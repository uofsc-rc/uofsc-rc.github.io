---
layout: default
title: Cuda Tutorial
---
<link rel="stylesheet" href="../css/main.css">
<div class="container">
    <div class="jumbotron header-img">
        <h1>Cuda Tutorial</h1>
        <p><a class="btn btn-primary btn-lg header-button"
              href="https://github.com/uofsc-rc/uofsc-rc.github.io/tree/main/jobs/CUDA">
            Find the files in this tutorial on our GitHub!</a></p>
    </div>
    <p>
        CUDA is a parallel computing platform and API model created by Nvidia that allows software developers
        to use a CUDA-enabled GPU for general purpose processing.
        The CUDA platform is designed to work with programming languages like C and C++.
        <a href="https://docs.nvidia.com/cuda/">Documentation for using CUDA can be found on its official website.</a>
    </p>
    <p>Currently, CUDA versions 7.5, 8.0, 9.0, 9.1, 10.0, 10.1, and 11.1  are available on the cluster.</p>
    <div>
        <h3 style="color:#537391;"><strong>Basics</strong></h3>
        <p>To run CUDA through the command line, first load the CUDA module, and create a CUDA file.
            To load the module, use the command:
        <p style="text-align: center;"><code>module load cuda</code></p></p>

        <p>To load a specific version of the modules, add the version number to the load command.
        <p style="text-align: center;"><code>module load cuda/8.0</code></p></p>

        <p>Compile the CUDA file using
        <p style="text-align: center;"><code>nvcc &lt;filename&gt;.cu -o &lt;filename&gt;</code></p></p>

        <p>where <i>&lt;filename&gt;</i> is the name of the CUDA file. nvcc separates the source code into host and device
            components.
            Device functions are processed by the NVIDIA compiler, and host functions are processed by the standard host
            compiler (like gcc).
            -o <i>&lt;filename&gt;</i> signals to create an output file called
            <i>&lt;filename&gt;</i> that can be used to run the program using ./<i>&lt;filename&gt;</i>. </p>
    </div>

    <div>
        <h3 style="color:#537391;"><strong>Running CUDA through a job script</strong></h3>
        <p>This tutorial was modified from a
            <a href="https://www.olcf.ornl.gov/wp-content/uploads/2013/02/Intro_to_CUDA_C-TS.pdf">NVIDIA CUDA
                tutorial</a></p>

        1. Create a CUDA script. This repository provides a simple script, <i>vector_add.cu</i>,
        which performs simple vector addition while utilizing blocks and threads to execute in parallel.

        <h4>vector_add.cu</h4>
        <pre><code>
#include &lt;stdio.h&gt;
#include &lt;iostream&gt;
//perform vector addition utilizing blocks and threads

__global__ void add(int *a, int *b, int *c, int n) {
        int index = threadIdx.x + blockIdx.x * blockDim.x;
        if (index < n) //avoid accessing beyond end of array
                c[index] = a[index] + b[index];

}

//populate vectors with random ints
void random_ints(int* a, int N) {
    for (int i=0; i < N; i++){
        a[i] = rand() % 1000;
    }
}

#define N (2048*2048) // overall size of the data set
#define THREADS_PER_BLOCK 512 // threads per block

int main(void) {
        int *a, *b, *c;
        int *d_a, *d_b, *d_c;
        int size = N * sizeof(int);

        //alloc space for device copies of a, b, and c
        cudaMalloc((void **)&d_a, size);
        cudaMalloc((void **)&d_b, size);
        cudaMalloc((void **)&d_c, size);

        //alloc space for host copies and setup input values
        a = (int *)malloc(size); random_ints(a, N);
        b = (int *)malloc(size); random_ints(b, N);
        c = (int *)malloc(size);

        //copy inputs to device
        cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);
        cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);

        //launch add() kernel, while avoid accessing beyond the end of the array
        add<<<(N + THREADS_PER_BLOCK-1)/THREADS_PER_BLOCK, THREADS_PER_BLOCK>>>(d_a, d_b, d_c, N);

        cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);

        //clean up
        free(a); free(b); free(c);
        cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);
        return 0;
}
        </code></pre>

        2. Compile the CUDA script using <code>nvcc vector_add.cu -o vector_add</code>
        which creates an executable called <i>vector_add</i>


        <p>3. Prepare the submission script, which is the script that is submitted to the
            Slurm scheduler as a job in order to run the CUDA script. This repository provides the script <i>job.sh</i>
            as an example.</p>
        <h4>job.sh</h4>
        <pre><code>
#!/bin/bash

#SBATCH --job-name=cuda_test
#SBATCH -o cuda_out%j.out
#SBATCH -e cuda_err%j.err
#SBATCH --gres=gpu:1


echo -e '\nsubmitted cuda job'
echo 'hostname'
hostname

#loads the cuda module
module load cuda

#recompiles the vector_add.cu file
nvcc vector_add.cu -o vector_add

#runs the vector_add program
./vector_add
			</code></pre>

        4. Submit the job using
        <code>sbatch job.sh</code>

        <p>5. Examine the results.</p>
    </div>
</div>

